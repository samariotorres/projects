{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used to initialize the neural network, because there are two ways of initializing\n",
    "a neural network, either as a sequence of layers or as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " used for the convultional step in the CNN; where we add the \n",
    " convolutional layers; since we are using images, we use the 2D class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used for step two; the pooling step; which will add our pooling layers so we can decrease the size of our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used for step 3; flattening; where we convert all of the pooled feature maps that\n",
    " we create through convolution and max pooling into a feature vector that will eventually become our input of\n",
    " our fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " used to add the fully connected layers in the classic neural network.\n",
    "\n",
    " Each package correspondes to one step of the construction of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential() # initializing the CNN as a sequence of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samariotorres/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32,3, 3, input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First argument is the number of filters; the number of filters that we choose is the number of feature maps \n",
    "that we want to create as well; i.e, one feature map created for each filter used. \n",
    "So this means we created 32 feature detectors of 3x3 dimensions and so our convolutional layer will be \n",
    "composed of 32 feature maps.\n",
    "\n",
    "Next parameter is border_mode ='same'; this is how we want to specify how the feature detectors will handle the\n",
    "borders of the input image, most of the time we choose 'same', so we leave it as default\n",
    "\n",
    "Next argument is input_shape; the shape of the input image on which you are going to apply your feature maps; we need to specify the expected format of our input images. Also need to be careful because the docs for this parameter say input_shape = (3,256,256) and thats the order for the THEANO backend; here we are using the TENSORFLOW backend; run the first cell to see. The order for the TENSORFLOW backend is actually input_shape = (256,256, 3) wher 256,256 is the dimension of our input array, and 3; where 3 is the number of channels; i.e, red,green,blue (it would only be 1 if we were dealing with black and white image)\n",
    "\n",
    "Last argument is the activation function which we will set as the Rectifier Activation Function (RELu) so that we make sure we dont have any negative pixel values in our feature maps. Dependeing on the parameters that we use for the convolution operation, we can get negative pixels in the feature map and we want to remove these negative pixels in order to have non-linearity in the CNN. Because classifying images is a non-linear problem and we need to have non-linearity in our model; so we use relu activation function to break up the linearit of the image. By linearity we mean like the flow of the colors; we want to break that up so that we can get some distict parts of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to apply the max pooling step so that we can reduce the number of nodes we'll get in the\n",
    "flattening step, and then the full connection step for the input layer of the feature NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basically this line will reduce the size of the feature maps # by dividing by 2. \n",
    "\n",
    "the size parameter, pool_size = (); the pool size i.e, is like how much you slide the matrix around \n",
    "\n",
    "Applying maxpooling to reduce the size of the feature maps and therefore reduce the number of nodes in the upcoming\n",
    "fully connected layers; it will reduce the complexity and the time execution w/o losing the performance.\n",
    "\n",
    "We are keeping track of the parts of the image that contain the high numbers corresponding to where the\n",
    "feature detectors detected some specific features in the input image; so we will not lose spatial structure  information and therefore we do not lose the performance of the model; and at the same we manage to reduce the \n",
    "time complexity and less expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The reason we are applying the MaxPooling step and the Convultional Layers is so that when we get to the final\n",
    "step, the flattening step, we dont have this huge 1D vector of input nodes that represents every single pixel in the image independently of the pixels that are around it.\n",
    "\n",
    "We want information of the spatial structure for the pixels; so we apply max pooling and convolutional layers; using\n",
    "our feature maps, we extracted spatial structure information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening step completed; a single vector was created that contains all of the \n",
    "information of the spatial structure of the images.\n",
    "\n",
    "Now the only thing left to do is to create FULLY CONNECTED a classic neural network that will classify the images;\n",
    "And it will classify them well thanks to this input vector (flattened matrix vector) that contains the information\n",
    "of the spatial structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samariotorres/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added the hidden layer; the fully connected layer.\n",
    "\n",
    "Using the Dense function to add a fully connected layer.\n",
    "\n",
    "The first parameter is output_dim is the number of nodes in the hidden layer; how many nodes should we input?\n",
    "Common practice is to choose a number between the number of hidden nodes and the number of output nodes; we are\n",
    "experimenting; also its common practice to pick a number of the form; where x is an integer,\n",
    "\n",
    "$$2^x$$ \n",
    "\n",
    "So we will go with x = 7; 128 hidden nodes in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to define the output layer using the activation function, sigmoid,\n",
    "because we have a binary outcome, so we say output_dim = 1, since we are only expecting one node that is going to \n",
    "be the predicted probability of one class; in this case, cat or dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samariotorres/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the CNN using the compile( ) method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam',loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SGD algorithm, adam algorithm, the loss function is going to be the binary_crossentropy, since this function\n",
    "corresponds to the logarithmic loss which is the loss function generally used for classification problems using \n",
    "a classification model like logistic regression; and also because we have a binary outcome.\n",
    "\n",
    "Note: If we had more than two outcomes, we would need to use categorical_crossentropy\n",
    "\n",
    "And the last parameter we will use is the performance metric; i.e, how we are measuring accurary (because thats\n",
    "what we want, we want to be accurate), so we set metrics = ['accuracy']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------Part 2: Fitting the CNN to the images-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a shortcut from the Keras documentation for Image Augmentation.\n",
    "\n",
    "Consists of pre-processing the images to prevent overfitting.\n",
    "\n",
    "If we do not do this image augmentation then we will end up with great accuracy results on the training set,\n",
    "but a much lower accuracy on the test set due to an overfit on the training set.\n",
    "\n",
    "The first function we are going to use to perform image augmentation can be found at the top of \n",
    "https://keras.io/preprocessing/image/\n",
    "\n",
    "Question: What is image augmentation? \n",
    "\n",
    "Answer: To begin with, one of the situations that lead to overfitting is when we have few data to train our \n",
    "model. When this occurs, the model finds correlations in the few observations of the training set but fails to\n",
    "GENERALIZE these correlations on some new set of observations.\n",
    "\n",
    "When it comes to images, we need a lot of images to find and generalize some correlations.\n",
    "\n",
    "In computer vision, our ML model does not simply need to find some correlations between some independent variables\n",
    "and dependent variables, it needs to find patterns in the pixels, and to do this requires many examples and test \n",
    "images.\n",
    "\n",
    "Data augmentation will create many batches of our images and in each batch it will apply some random transformations\n",
    "on a random selection of our images. Like rotating, flipping, shifting, or even shearing(pushing) the images. \n",
    "\n",
    "What we will end up with is many more diverse images inside these batches and therefore a lot more images to train.\n",
    "\n",
    "That's why its called, image augmentation; because the training images are augmented.\n",
    "\n",
    "And since the transformations are random transformations the model will never find the same picture across the \n",
    "batches.\n",
    "\n",
    "Summary: Image augmentation is a technique that allows us to enrich our training set without adding more images\n",
    "and therefore allows us to get good performance results with little to no overfitting, even with a small number of\n",
    "images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was copy and pasted(with a few name and number edits) from https://keras.io/preprocessing/image/\n",
    "under the .flow_from_directory(directory) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the ImageDataGenerator class below to perform the image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are rescaling all of the pixel values. \n",
    "\n",
    "Pixels take values between 0 and 255 and by rescaling them with ${1/255}$, then all of the pixel values \n",
    "will be between 0 and 1; for sigmoid purposes.\n",
    "\n",
    "Shear_range is to apply random transvections, we'll set it equal to 0.2\n",
    "\n",
    "Zoom_range is to apply random zooms, we'll set it equal to 0.2\n",
    "\n",
    "The 0.2 values are just how much we want to apply these random transformations.\n",
    "\n",
    "Horizontal flip will flip the images horizontally so that we don't find the same image in the different batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator( \n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line below, we are doing the same thing we just did to the training set, except now to the test_set\n",
    "and we only need to rescale and leave everything else as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are beginning by setting the directory to where the data is located.\n",
    "For example, the reason that it only says 'dataset/training_set' is because this jupyter notebook that I am currently \n",
    "typing is inside the same directory so its very simple. But if you need to, just modify the directory below to where\n",
    "the cats and dogs training and test sets are located.\n",
    "\n",
    "No we are going to specify the target_size; it is the size of the images expected in the CNN model; (64,64,3); but\n",
    "just the size, so (64,64)\n",
    "\n",
    "Then the batch_size; it is the size of the batches in which random samples of our images will be included and contains the number of images that will go through the CNN after which the weights will be updated.\n",
    "We'll let it equal 32 to train our CNN. (The weights get updated after every batch).\n",
    "\n",
    "Lastly is class_mode; its the parameter indicating if your dependent variable is binary or has more than two \n",
    "catgories and since we have cats and dogs, which is two, we'll let the class_mode = 'binary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size = (64, 64),\n",
    "        batch_size = 32, \n",
    "        class_mode ='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above where it says \"Found 8000 images belonging to 2 classes.\" when you run that block, it says that because\n",
    "of how nice we set up the folder. When you're classifying, make sure the folder are organized :)\n",
    "Similar parameter values for the test_set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model.fit_generator function below: (model in our case is the classifier object from above)\n",
    "\n",
    "The first argument is the training set so we say training_set.\n",
    "\n",
    "Second parameter is the samples per epoch; which is the number of images we have in our training set.\n",
    "Because all of the observations of the training set pass through the convolutional neural network during each epoch,\n",
    "and since we have 8000 images our training set we need to set steps_per_epoch = 8000.\n",
    "\n",
    "Then the third parameter, epochs; the number of epochs we want to choose to train our CNN. We'll let it be 25;\n",
    "But you could choose a larger number; it depends on how long you're willing to wait.\n",
    "\n",
    "Then the fourth parameter, validation_data; corresponds to the test set on which we want to evaluate the \n",
    "performance of our CNN. So validation_data = test_set.\n",
    "\n",
    "And finally, the validation_steps; corresponds to the number of images in our test set which is 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will take 20 minutes to run on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch = 8000,\n",
    "        epochs = 1,\n",
    "        validation_data = test_set,\n",
    "        validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make our model more accurate. \n",
    "\n",
    "So we are going to increase the wideness; increase the number of hidden layers; make the model deeper.\n",
    "\n",
    "How can we make it deeper? \n",
    "\n",
    "Two options: \n",
    "\n",
    "First option: Add another convolutional layer:\n",
    "\n",
    "classifier.add(Convolution2D(32,3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "Second Option: Add another fully connected layer:\n",
    "\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "\n",
    "The best solution is to add another convolutional layer:\n",
    "\n",
    "classifier.add(Convolution2D(32,3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "But you can always do both; which one works faster with greatest efficacy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's all of the code in action with ANOTHER convolutional layer!\n",
    "\n",
    "Goal: CNN with two convolutional layers; get a test set accuracy > 80%. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samariotorres/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/samariotorres/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  import sys\n",
      "/Users/samariotorres/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "  del sys.path[0]\n",
      "/Users/samariotorres/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 1243s 155ms/step - loss: 0.3878 - acc: 0.8162 - val_loss: 0.4747 - val_acc: 0.8119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb296343c8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential() # initializing the CNN as a sequence of layers.\n",
    "\n",
    "classifier.add(Convolution2D(32,3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(32,3, 3, activation = 'relu')) #2nd Convolutional Layer, doesn't need input_shape param\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2))) #2nd Convolutional Layer; have to Pool after convolution.\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam',loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator( \n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size = (64, 64),\n",
    "        batch_size = 32,\n",
    "        class_mode ='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64), \n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch = 8000,\n",
    "        epochs = 1,\n",
    "        validation_data = test_set,\n",
    "        validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to improve the model even more, then you could choose a higher target_size for the images in the training_set and test_set functions so that you get more information about the pixel patterns.\n",
    "Because if you increase the size of the images, then you will get more rows and more columns in the input images,\n",
    "and therefore there will be more information to take on the pixels. \n",
    "(GPU Recommended (don't do it to your stock macbook like I did.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test an image run the following cell: We use 0.5 as our threshold because by convention thats whats used when\n",
    "the activation function for the last hidden layer is sigmoid and we only have 2 categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'random.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f67cdc1c4a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'random.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    497\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'random.jpg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('random.jpg', target_size = (64,64)) #input a number from the dataset for random\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] >= 0.5:\n",
    "    prediction = 'dog'\n",
    "else: \n",
    "    prediction = 'cat'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
